{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch_cluster import random_walk\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import NeighborSampler as RawNeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "dataset = EllipticBitcoinDataset(root='./pytorch_input')\n",
    "data=dataset[0]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#70/30 split, first 34 timestep, occurs at index 136265\n",
    "newsplit =  torch.Tensor([True]*136265+[False]*67504)\n",
    "data.train_mask = torch.logical_and(data.y <2,newsplit)\n",
    "data.test_mask = torch.logical_and(data.y <2,torch.logical_not(newsplit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 64])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [203769] at index 0 does not match the shape of the indexed tensor [768, 64] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=90'>91</a>\u001b[0m \u001b[39m# def test():\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=91'>92</a>\u001b[0m \u001b[39m#     model.eval()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=92'>93</a>\u001b[0m \u001b[39m#     out = model.full_forward(x, edge_index).cpu()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=98'>99</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=99'>100</a>\u001b[0m \u001b[39m#     return train_acc, test_acc\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=102'>103</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m51\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=103'>104</a>\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=104'>105</a>\u001b[0m     train_acc, test_acc \u001b[39m=\u001b[39m test()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=105'>106</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=106'>107</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb Cell 3'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=66'>67</a>\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=67'>68</a>\u001b[0m \u001b[39m# out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=68'>69</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=69'>70</a>\u001b[0m \u001b[39m# pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=70'>71</a>\u001b[0m \u001b[39m# neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=71'>72</a>\u001b[0m \u001b[39m# loss = -pos_loss - neg_loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=72'>73</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(out[data\u001b[39m.\u001b[39;49mtrain_mask], data\u001b[39m.\u001b[39my[data\u001b[39m.\u001b[39mtrain_mask],weight\u001b[39m=\u001b[39mclassweight)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=73'>74</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhuaxu/Desktop/SophomoreSummer/GraphAI/GraphSAGE.ipynb#ch0000001?line=74'>75</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [203769] at index 0 does not match the shape of the indexed tensor [768, 64] at index 0"
     ]
    }
   ],
   "source": [
    "class NeighborSampler(RawNeighborSampler):\n",
    "    def sample(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "        row, col, _ = self.adj_t.coo()\n",
    "\n",
    "        # For each node in `batch`, we sample a direct neighbor (as positive\n",
    "        # example) and a random node (as negative example):\n",
    "        pos_batch = random_walk(row, col, batch, walk_length=1,\n",
    "                                coalesced=False)[:, 1]\n",
    "\n",
    "        neg_batch = torch.randint(0, self.adj_t.size(1), (batch.numel(), ),\n",
    "                                  dtype=torch.long)\n",
    "\n",
    "        batch = torch.cat([batch, pos_batch, neg_batch], dim=0)\n",
    "        return super().sample(batch)\n",
    "\n",
    "\n",
    "train_loader = NeighborSampler(data.edge_index, sizes=[10, 10], batch_size=256,\n",
    "                               shuffle=True, num_nodes=data.num_nodes)\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(SAGEConv(in_channels, hidden_channels,aggr=\"mean\"))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def full_forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SAGE(data.num_node_features, hidden_channels=64, num_layers=3)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=5e-4)\n",
    "x, edge_index = data.x.to(device), data.edge_index.to(device)\n",
    "classweight = torch.FloatTensor([0.3,0.7,0])\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        #print(adjs)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(x[n_id], adjs)\n",
    "        print(out.shape)\n",
    "        # out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
    "\n",
    "        # pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
    "        # neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
    "        # loss = -pos_loss - neg_loss\n",
    "        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask],weight=classweight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * out.size(0)\n",
    "\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.test_mask]:\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n",
    "# def test():\n",
    "#     model.eval()\n",
    "#     out = model.full_forward(x, edge_index).cpu()\n",
    "#     clf = LogisticRegression()\n",
    "#     clf.fit(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "#     train_acc = clf.score(out[data.train_mask], data.y[data.train_mask])\n",
    "#     test_acc = clf.score(out[data.test_mask], data.y[data.test_mask])\n",
    "\n",
    "#     return train_acc, test_acc\n",
    "\n",
    "\n",
    "for epoch in range(1,51):\n",
    "    loss = train()\n",
    "    train_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train: {train_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15504    83]\n",
      " [ 1062    21]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "out = model.full_forward(x, edge_index).cpu()\n",
    "clf = LogisticRegression()\n",
    "clf.fit(out[data.train_mask].detach().numpy(), data.y[data.train_mask])\n",
    "pred = clf.predict(out[data.test_mask].detach().numpy())\n",
    "print(confusion_matrix(data.y[data.test_mask],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03, Loss: 0.1767, Approx. Train: 0.9610\n",
      "Epoch: 05, Train: 0.9799,Test: 0.9630\n",
      "Epoch 08, Loss: 0.1286, Approx. Train: 0.9689\n",
      "Epoch: 10, Train: 0.9841,Test: 0.9557\n",
      "Epoch 13, Loss: 0.1141, Approx. Train: 0.9732\n",
      "Epoch: 15, Train: 0.9875,Test: 0.9647\n",
      "Epoch 18, Loss: 0.1110, Approx. Train: 0.9728\n",
      "Epoch: 20, Train: 0.9871,Test: 0.9624\n",
      "Epoch 23, Loss: 0.1837, Approx. Train: 0.9726\n",
      "Epoch: 25, Train: 0.9884,Test: 0.9523\n",
      "Epoch 28, Loss: 0.0943, Approx. Train: 0.9787\n",
      "Epoch: 30, Train: 0.9908,Test: 0.9650\n",
      "Epoch 33, Loss: 0.0907, Approx. Train: 0.9798\n",
      "Epoch: 35, Train: 0.9911,Test: 0.9652\n",
      "Epoch 38, Loss: 0.0676, Approx. Train: 0.9824\n",
      "Epoch: 40, Train: 0.9910,Test: 0.9639\n",
      "Epoch 43, Loss: 0.0824, Approx. Train: 0.9803\n",
      "Epoch: 45, Train: 0.9897,Test: 0.9657\n",
      "Epoch 48, Loss: 0.0790, Approx. Train: 0.9795\n",
      "Epoch: 50, Train: 0.9913,Test: 0.9655\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "import copy\n",
    "kwargs = {'batch_size': 1024, 'num_workers': 6, 'persistent_workers': True}\n",
    "train_loader = NeighborLoader(data, input_nodes=data.train_mask,\n",
    "                              num_neighbors=[25, 10], shuffle=True, **kwargs)\n",
    "\n",
    "subgraph_loader = NeighborLoader(copy.copy(data), input_nodes=None,\n",
    "                                 num_neighbors=[-1], shuffle=False, **kwargs)\n",
    "\n",
    "# No need to maintain these features during evaluation:\n",
    "del subgraph_loader.data.x, subgraph_loader.data.y\n",
    "# Add global node index information.\n",
    "subgraph_loader.data.num_nodes = data.num_nodes\n",
    "subgraph_loader.data.n_id = torch.arange(data.num_nodes)\n",
    "\n",
    "classweight = torch.FloatTensor([0.3,0.7])\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        # self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = x.relu_()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x_all, subgraph_loader):\n",
    "        #pbar = tqdm(total=len(subgraph_loader.dataset) * len(self.convs))\n",
    "        #pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch:\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch in subgraph_loader:\n",
    "                x = x_all[batch.n_id.to(x_all.device)].to(device)\n",
    "                x = conv(x, batch.edge_index.to(device))\n",
    "                if i < len(self.convs) - 1:\n",
    "                    x = x.relu_()\n",
    "                xs.append(x[:batch.batch_size].cpu())\n",
    "                #pbar.update(batch.batch_size)\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "        #pbar.close()\n",
    "        return x_all\n",
    "\n",
    "\n",
    "model = SAGE(dataset.num_features, 256, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01,weight_decay=5e-5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    #pbar = tqdm(total=int(len(train_loader.dataset)))\n",
    "    #pbar.set_description(f'Epoch {epoch:02d}')\n",
    "\n",
    "    total_loss = total_correct = total_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y = batch.y[:batch.batch_size]\n",
    "        y_hat = model(batch.x, batch.edge_index.to(device))[:batch.batch_size]\n",
    "        loss = F.cross_entropy(y_hat, y,weight=classweight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * batch.batch_size\n",
    "        total_correct += int((y_hat.argmax(dim=-1) == y).sum())\n",
    "        total_examples += batch.batch_size\n",
    "        #pbar.update(batch.batch_size)\n",
    "    #pbar.close()\n",
    "\n",
    "    return total_loss / total_examples, total_correct / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    y_hat = model.inference(data.x, subgraph_loader).argmax(dim=-1)\n",
    "    y = data.y.to(y_hat.device)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.test_mask]:\n",
    "        accs.append(int((y_hat[mask] == y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n",
    "\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    loss, acc = train(epoch)\n",
    "    if epoch % 5 == 3:\n",
    "        print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n",
    "    train_acc, test_acc = test()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:02d}, Train: {train_acc:.4f},'\n",
    "            f'Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15490    97]\n",
      " [  478   605]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "y_hat = model.inference(data.x, subgraph_loader).argmax(dim=-1)\n",
    "print(confusion_matrix(data.y[data.test_mask],y_hat[data.test_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15587\n",
       "1     1083\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(data.y[data.test_mask]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9655068986202759"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(97+478)/(15587+1083)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ab251e2c80fb6e00571266a93872e1e3c37260c6a2bb64773851c985a7be3c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
